{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect English loanword candidates\n",
    "In which we collect the loanword candidates that are specific to the social media platforms of interest, using previously collected word counts (\"fetch\" study) and POS tags (to isolate noun/verb pairs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get POS tags (reddit)\n",
    "Let's get the POS tags for reddit using the \"fetch\" study tags as a starting point. We'll expand to a bigger vocabulary if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>#</th>\n",
       "      <th>$</th>\n",
       "      <th>&amp;</th>\n",
       "      <th>,</th>\n",
       "      <th>A</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>G</th>\n",
       "      <th>L</th>\n",
       "      <th>...</th>\n",
       "      <th>R</th>\n",
       "      <th>S</th>\n",
       "      <th>T</th>\n",
       "      <th>U</th>\n",
       "      <th>V</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "      <th>^</th>\n",
       "      <th>~</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>nunnery</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003542</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>sowell</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.991653</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>woods</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>clotted</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.99088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004951</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>spiders</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           !    #    $    &    ,        A    D    E    G    L  ...    R    S  \\\n",
       "nunnery  0.0  0.0  0.0  0.0  0.0  0.00000  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "sowell   0.0  0.0  0.0  0.0  0.0  0.00000  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "woods    0.0  0.0  0.0  0.0  0.0  0.00000  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "clotted  0.0  0.0  0.0  0.0  0.0  0.99088  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "spiders  0.0  0.0  0.0  0.0  0.0  0.00000  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "\n",
       "           T    U         V    X    Y    Z         ^    ~  \n",
       "nunnery  0.0  0.0  0.000000  0.0  0.0  0.0  0.003542  0.0  \n",
       "sowell   0.0  0.0  0.003641  0.0  0.0  0.0  0.991653  0.0  \n",
       "woods    0.0  0.0  0.000026  0.0  0.0  0.0  0.000018  0.0  \n",
       "clotted  0.0  0.0  0.002703  0.0  0.0  0.0  0.004951  0.0  \n",
       "spiders  0.0  0.0  0.000112  0.0  0.0  0.0  0.000682  0.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reddit_tag_pcts = pd.read_csv('../../data/mined_reddit_comments/reddit_2013_2016_tag_pcts.tsv', sep='\\t', index_col=0)\n",
    "display(reddit_tag_pcts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test word post has tag pcts\n",
      "N    0.714383\n",
      "V    0.284010\n",
      "A    0.001562\n",
      "P    0.000024\n",
      "R    0.000021\n",
      "Name: post, dtype: float64\n",
      "test word upvote has tag pcts\n",
      "V    5.824138e-01\n",
      "N    3.742320e-01\n",
      "A    2.004457e-02\n",
      "G    1.407947e-02\n",
      "!    4.537287e-03\n",
      "^    3.788560e-03\n",
      "R    5.133337e-04\n",
      "O    2.714444e-04\n",
      "P    1.045992e-04\n",
      "E    8.919910e-06\n",
      "D    5.498812e-06\n",
      "T    5.552668e-07\n",
      "Name: upvote, dtype: float64\n",
      "test word downvote has tag pcts\n",
      "V    0.666391\n",
      "N    0.280947\n",
      "A    0.035475\n",
      "^    0.008346\n",
      "R    0.005686\n",
      "!    0.002126\n",
      "G    0.000623\n",
      "P    0.000153\n",
      "D    0.000152\n",
      "O    0.000095\n",
      "T    0.000007\n",
      "Name: downvote, dtype: float64\n",
      "test word block has tag pcts\n",
      "N    0.583007\n",
      "V    0.415149\n",
      "A    0.001771\n",
      "^    0.000068\n",
      "R    0.000005\n",
      "Name: block, dtype: float64\n",
      "test word troll has tag pcts\n",
      "N    0.950627\n",
      "V    0.043040\n",
      "^    0.005972\n",
      "A    0.000336\n",
      "R    0.000025\n",
      "Name: troll, dtype: float64\n",
      "test word trolled has tag pcts\n",
      "V    0.989717\n",
      "N    0.005804\n",
      "A    0.004478\n",
      "Name: trolled, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# let's try some candidate noun/verb pairs\n",
    "test_words = ['post', 'upvote', 'downvote', 'block', 'troll', 'trolled']\n",
    "for test_word in test_words:\n",
    "    tag_pct_i = reddit_tag_pcts.loc[test_word, :].sort_values(inplace=False, ascending=False)\n",
    "    tag_pct_i = tag_pct_i[tag_pct_i > 0.]\n",
    "    print('test word %s has tag pcts\\n%s'%(test_word, tag_pct_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these make sense, except for `troll` which I would expect to have a higher verb distribution.\n",
    "\n",
    "Maybe a better way to find verb candidates is to look in the `EN` vocab for consistent groups of verbs based on suffixes (`trolling`, `trolled`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic test for finding verb/noun pairs:\n",
    "\n",
    "- find words with high likelihood of being noun (>=50%)\n",
    "- look for verb matches in vocab (`troll` => `trolled`, etc.)\n",
    "- if # verb matches > 0, then pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "addi                0.250883\n",
       "dosing              0.250882\n",
       "wikibot             0.250859\n",
       "cruces              0.250857\n",
       "unicums             0.250849\n",
       "tomoko              0.250827\n",
       "pubstomping         0.250752\n",
       "bristled            0.250731\n",
       "trichotillomania    0.250693\n",
       "zl                  0.250677\n",
       "journaled           0.250676\n",
       "tanaan              0.250614\n",
       "oxidizes            0.250594\n",
       "cannonballs         0.250593\n",
       "waluigi             0.250561\n",
       "clippers            0.250542\n",
       "relabeled           0.250533\n",
       "faints              0.250524\n",
       "stater              0.250523\n",
       "whirls              0.250455\n",
       "daggerfall          0.250450\n",
       "booom               0.250448\n",
       "corsair             0.250437\n",
       "aldmeri             0.250432\n",
       "neuvy               0.250401\n",
       "chronomancer        0.250389\n",
       "fcp                 0.250384\n",
       "vodlocker           0.250383\n",
       "urs                 0.250381\n",
       "romanticizing       0.250337\n",
       "andro               0.250323\n",
       "srd                 0.250304\n",
       "frodan              0.250298\n",
       "radeon              0.250259\n",
       "karmas              0.250246\n",
       "decant              0.250240\n",
       "benetton            0.250240\n",
       "immolate            0.250229\n",
       "havasu              0.250212\n",
       "recharges           0.250178\n",
       "catcall             0.250169\n",
       "salarian            0.250152\n",
       "thps                0.250145\n",
       "falkenrath          0.250139\n",
       "rti                 0.250041\n",
       "taniks              0.250030\n",
       "blir                0.250016\n",
       "shilled             0.250003\n",
       "revives             0.250001\n",
       "crot                0.250000\n",
       "Name: N, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "noun_cutoff = 0.25\n",
    "noun_candidate_tag_pcts = reddit_tag_pcts[reddit_tag_pcts.loc[:, 'N'] >= noun_cutoff].loc[:, 'N'].sort_values(inplace=False, ascending=False)\n",
    "display(noun_candidate_tag_pcts.tail(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure! Let's look for some noun/verb pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "verb_inflections = ['ed', 'ing']\n",
    "# handle words with word-final <e>\n",
    "noun_ending_matcher = re.compile('e$')\n",
    "# TODO: handle short-vowel words? \"bat\" => \"batting\"\n",
    "noun_candidates = noun_candidate_tag_pcts.index\n",
    "en_vocab = reddit_tag_pcts.index.tolist()\n",
    "noun_verb_candidates = []\n",
    "for noun_candidate in noun_candidates:\n",
    "    # handle words with word-final <e>\n",
    "    if(noun_ending_matcher.search(noun_candidate)):\n",
    "        noun_candidate_stem = noun_ending_matcher.sub('', noun_candidate)\n",
    "        generated_verbs = ['%s%s'%(noun_candidate_stem, verb_inflection) for verb_inflection in verb_inflections]\n",
    "    else:\n",
    "        generated_verbs = ['%s%s'%(noun_candidate, verb_inflection) for verb_inflection in verb_inflections]\n",
    "    valid_generated_verbs = list(filter(lambda x: x in en_vocab, generated_verbs))\n",
    "    if(len(valid_generated_verbs) > 0):\n",
    "        noun_verb_candidate_pair = [noun_candidate, valid_generated_verbs]\n",
    "        noun_verb_candidates.append(noun_verb_candidate_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['damme', ['dammed', 'damming']], ['parte', ['parted', 'parting']], ['people', ['peopled']], ['teeth', ['teething']], ['fad', ['faded', 'fading']], ['superpower', ['superpowered']], ['mode', ['moded', 'moding']], ['football', ['footballing']], ['sugar', ['sugared', 'sugaring']], ['tooth', ['toothed']], ['mouth', ['mouthed', 'mouthing']], ['proportion', ['proportioned']], ['conference', ['conferencing']], ['fate', ['fated']], ['hair', ['haired']], ['panel', ['paneling']], ['intention', ['intentioned']], ['event', ['evented', 'eventing']], ['course', ['coursing']], ['neighbor', ['neighboring']]]\n"
     ]
    }
   ],
   "source": [
    "print(noun_verb_candidates[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to dict for easy lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_verb_candidates_lookup = {k : v for k,v in noun_verb_candidates}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['downvote', 'downvoters', 'downvotes', 'downvoter', 'downvoted']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in en_vocab if 'downvote' in str(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun post verbs ['posted', 'posting']\n",
      "noun upvote verbs ['upvoted', 'upvoting']\n",
      "noun downvote verbs ['downvoted', 'downvoting']\n",
      "noun block verbs ['blocked', 'blocking']\n",
      "noun troll verbs ['trolled', 'trolling']\n"
     ]
    }
   ],
   "source": [
    "# check for manual pairs\n",
    "test_words = ['post', 'upvote', 'downvote', 'block', 'troll']\n",
    "for test_word in test_words:\n",
    "    if(test_word) in noun_verb_candidates_lookup:\n",
    "        print('noun %s verbs %s'%(test_word, noun_verb_candidates_lookup[test_word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! We have a valid filtering strategy, as long as we're OK with a relatively low `N`/`V` threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@user     @\n",
      ">         E\n",
      "<         ~\n",
      ":         ~\n",
      "url       N\n",
      "rt        ~\n",
      ".         ,\n",
      "num       ^\n",
      "#hash     N\n",
      ",         ,\n",
      "…         ,\n",
      "!         ,\n",
      "\"         ,\n",
      "?         ,\n",
      "-         ,\n",
      "just      R\n",
      "...       ,\n",
      "like      P\n",
      "&         &\n",
      "will      V\n",
      "(         ,\n",
      "one       $\n",
      "get       V\n",
      "can       V\n",
      "people    N\n",
      ")         ,\n",
      "love      V\n",
      "now       R\n",
      "“         ,\n",
      "new       A\n",
      "time      N\n",
      "'         ,\n",
      "know      V\n",
      "”         ,\n",
      "good      A\n",
      "day       N\n",
      "need      V\n",
      "see       V\n",
      "want      V\n",
      "u         O\n",
      "i’m       L\n",
      "go        V\n",
      "today     N\n",
      "us        O\n",
      "best      A\n",
      "don’t     V\n",
      "really    R\n",
      ">.<       E\n",
      "back      R\n",
      "make      V\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse\n",
    "# combined\n",
    "# POS_counts_data = scipy.sparse.load_npz('../../data/mined_tweets/POS_tag_stats/tweets_POS_tags_LANG=en.npz')\n",
    "# POS_count_rows = [l.strip() for l in open('../../data/mined_tweets/POS_tag_stats/tweets_POS_tags_LANG=en_rows.txt', 'r')]\n",
    "# POS_count_cols = [l.strip() for l in open('../../data/mined_tweets/POS_tag_stats/tweets_POS_tags_LANG=en_cols.txt', 'r')]\n",
    "# sample\n",
    "POS_counts_data = scipy.sparse.load_npz('../../data/mined_tweets/POS_tag_stats/tweets-Aug-01-17-03-51_LANG=en_tagged_tag_pct.npz')\n",
    "POS_count_rows = [l.strip() for l in open('../../data/mined_tweets/POS_tag_stats/tweets-Aug-01-17-03-51_LANG=en_tagged_tag_pct_rows.txt', 'r')]\n",
    "POS_count_cols = [l.strip() for l in open('../../data/mined_tweets/POS_tag_stats/tweets-Aug-01-17-03-51_LANG=en_tagged_tag_pct_cols.txt', 'r')]\n",
    "# convert to dataframe\n",
    "POS_counts = pd.DataFrame(POS_counts_data.todense(), index=POS_count_rows, columns=POS_count_cols)\n",
    "POS_max_count_tags = POS_counts.idxmax(axis=1)\n",
    "print(POS_max_count_tags.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@user        1.0\n",
      ">            1.0\n",
      "<            1.0\n",
      ":            1.0\n",
      "url          1.0\n",
      "            ... \n",
      "ypu          1.0\n",
      "bargains     1.0\n",
      "barker       1.0\n",
      "you✅🐶        1.0\n",
      "daydreams    1.0\n",
      "Length: 32278, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# a = POS_counts.head(20) / POS_counts.head(20).sum(axis=1)\n",
    "a = POS_counts.copy()\n",
    "print(a.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>#</th>\n",
       "      <th>S</th>\n",
       "      <th>P</th>\n",
       "      <th>A</th>\n",
       "      <th>T</th>\n",
       "      <th>^</th>\n",
       "      <th>E</th>\n",
       "      <th>V</th>\n",
       "      <th>$</th>\n",
       "      <th>...</th>\n",
       "      <th>O</th>\n",
       "      <th>Z</th>\n",
       "      <th>&amp;</th>\n",
       "      <th>~</th>\n",
       "      <th>@</th>\n",
       "      <th>D</th>\n",
       "      <th>G</th>\n",
       "      <th>U</th>\n",
       "      <th>N</th>\n",
       "      <th>,</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>&lt;</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.481052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&gt;</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041749</td>\n",
       "      <td>0.728516</td>\n",
       "      <td>1.174961e-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.672169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.773987e-07</td>\n",
       "      <td>0.055594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>@user</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999744</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>:</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.256671</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.070761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>url</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.874806e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.022950e-01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         X    #    S    P         A    T         ^         E             V  \\\n",
       "<      0.0  0.0  0.0  0.0  0.000002  0.0  0.000080  0.000026  0.000000e+00   \n",
       ">      0.0  0.0  0.0  0.0  0.000003  0.0  0.041749  0.728516  1.174961e-06   \n",
       "@user  0.0  0.0  0.0  0.0  0.000000  0.0  0.000000  0.000000  0.000000e+00   \n",
       ":      0.0  0.0  0.0  0.0  0.000000  0.0  0.000000  0.000862  0.000000e+00   \n",
       "url    0.0  0.0  0.0  0.0  0.000000  0.0  0.000199  0.000000  5.874806e-07   \n",
       "\n",
       "         $  ...    O    Z    &         ~         @    D         G    U  \\\n",
       "<      0.0  ...  0.0  0.0  0.0  0.481052  0.000000  0.0  0.000209  0.0   \n",
       ">      0.0  ...  0.0  0.0  0.0  0.000016  0.000000  0.0  0.672169  0.0   \n",
       "@user  0.0  ...  0.0  0.0  0.0  0.000000  0.999744  0.0  0.000000  0.0   \n",
       ":      0.0  ...  0.0  0.0  0.0  0.256671  0.000000  0.0  0.000003  0.0   \n",
       "url    0.0  ...  0.0  0.0  0.0  0.000000  0.000000  0.0  0.000080  0.0   \n",
       "\n",
       "                  N         ,  \n",
       "<      0.000000e+00  0.000000  \n",
       ">      2.773987e-07  0.055594  \n",
       "@user  0.000000e+00  0.000000  \n",
       ":      0.000000e+00  0.070761  \n",
       "url    2.022950e-01  0.000000  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(POS_counts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8.477182e+17</td>\n",
       "      <td>[@MinorSmile09/@, Whenst/O, will/V, NibbAS/^, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.477182e+17</td>\n",
       "      <td>[RT/~, @BYDOYOUNG/@, :/~, 도영/N, 음색/N, -&gt;/G, ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.477182e+17</td>\n",
       "      <td>[도영/^, 공명/^, -&gt;/G, https://t.co/CLHHOaV1TY/U]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8.477182e+17</td>\n",
       "      <td>[도영/^, 움짤/^, -&gt;/G, https://t.co/stPDn1b5ta/U]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8.477182e+17</td>\n",
       "      <td>[RT/~, @wordstionary/@, :/~, https://t.co/UNwI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0                                                  1\n",
       "0  8.477182e+17  [@MinorSmile09/@, Whenst/O, will/V, NibbAS/^, ...\n",
       "1  8.477182e+17  [RT/~, @BYDOYOUNG/@, :/~, 도영/N, 음색/N, ->/G, ht...\n",
       "2  8.477182e+17      [도영/^, 공명/^, ->/G, https://t.co/CLHHOaV1TY/U]\n",
       "3  8.477182e+17      [도영/^, 움짤/^, ->/G, https://t.co/stPDn1b5ta/U]\n",
       "4  8.477182e+17  [RT/~, @wordstionary/@, :/~, https://t.co/UNwI..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## fixing EN tag file format\n",
    "import re\n",
    "import os\n",
    "from ast import literal_eval\n",
    "\n",
    "en_tag_file_matcher = re.compile('LANG=en')\n",
    "en_tag_data_dir = '/hg190/corpora/twitter-crawl/new-archive/POS_tags/'\n",
    "en_tag_files = list(filter(lambda x: en_tag_file_matcher.search(x) is not None, os.listdir(en_tag_data_dir)))\n",
    "en_tag_files = list(map(lambda x: os.path.join(en_tag_data_dir, x), en_tag_files))\n",
    "for f in en_tag_files:\n",
    "    df = pd.read_csv(f, sep='\\t', index_col=False, compression='gzip', converters={'1' : literal_eval})\n",
    "    display(df.head())\n",
    "    break\n",
    "#     df = df.assign(**{\n",
    "#         '1' : df.loc[:, '1'].apply(lambda x: list(map(lambda y: y.split('/'))))\n",
    "#     })\n",
    "# print(en_tag_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>903172109370843136</td>\n",
       "      <td>[(rt, ~), (@USER, @), (:, ~), (q, N), (:, ,), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>903172109396037632</td>\n",
       "      <td>[(@USER, @), (u, O), (r, V), (a, D), (good, A)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>903172109387665408</td>\n",
       "      <td>[(i'm, L), (the, D), (biggest, A), (baby, N), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>903172109404225537</td>\n",
       "      <td>[(rt, ~), (@USER, @), (:, ~), (if, P), (two, $...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>903172109404446721</td>\n",
       "      <td>[(and, &amp;), (i, O), (ain't, V), (going, V), (to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0                                                  1\n",
       "0  903172109370843136  [(rt, ~), (@USER, @), (:, ~), (q, N), (:, ,), ...\n",
       "1  903172109396037632  [(@USER, @), (u, O), (r, V), (a, D), (good, A)...\n",
       "2  903172109387665408  [(i'm, L), (the, D), (biggest, A), (baby, N), ...\n",
       "3  903172109404225537  [(rt, ~), (@USER, @), (:, ~), (if, P), (two, $...\n",
       "4  903172109404446721  [(and, &), (i, O), (ain't, V), (going, V), (to..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = '/hg190/corpora/twitter-crawl/new-archive/POS_tags/tweets-Sep-01-17-03-43_LANG=en_tagged.tsv.gz'\n",
    "df = pd.read_csv(f, sep='\\t', index_col=False, compression='gzip', converters={'1' : literal_eval})\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.loc[:, '1'].apply(lambda x: ' '.join(['//'.join(y) for y in x])).values[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['rt//~ @USER//@ ://~ q//N ://, photo//N vs//P video//N ?//, nj//^ ://, photo//N jin//^ ://, photo//N yg//N ://, photo//N hs//N ://, video//N jm//G ://~ video//N v://G video//N jk//G ://~ video//N',\n",
       "       '@USER//@ u//O r//V a//D good//A soul//N .//, 😁👍//E',\n",
       "       \"i'm//L the//D biggest//A baby//N when//R i'm//L sick//A and//& i//O annoy//V the//D shit//N out//P of//P everyone//N so//P please//V bare//A with//P me//O .//,\",\n",
       "       \"rt//~ @USER//@ ://~ if//P two//$ people//N are//V meant//V to//P be//V together//R ,//, they//O will//V eventually//R find//V their//D way//N back//R into//P each//D other's//S arms//N ,//, no//D matter//N what//O .//,\",\n",
       "       \"and//& i//O ain't//V going//V to//P war//V about//P no//D nigga//N unless//P it's//L my//D brothers//N nigga//N\",\n",
       "       'girls//N naked//A and//& nude//A having//V a//D sex//N naked//A man//N festival//N japan//^ <//~ URL//N >//,',\n",
       "       'block//N hash//N <//~ NUM//^ >//G adab//G <//~ NUM//^ >//G d//G <//~ NUM//^ >//G ff//G <//~ NUM//^ >//G c//G <//~ NUM//^ >//G b//V <//~ NUM//^ >//G d//G <//~ NUM//^ >//G ffced//V',\n",
       "       '@USER//@ long//A taped//V a//D spoon//N to//P his//D tongue//N using//V the//D power//N of//P breaking//V plates//N .//,',\n",
       "       'rt//~ yan//! ://, #HASH//N <//~ URL//N >//E',\n",
       "       'rt//~ @USER//@ ://~ book//N was//V an//D allegory//N for//P how//R damaging//A patriarchal//A society//N can//V be&how//V male//A violence//N &//& toxic//A masculinity//N can//V have//V dead//A …//,'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 215)\t1\n",
      "  (0, 22)\t1\n",
      "  (0, 14)\t3\n",
      "  (0, 206)\t1\n",
      "  (0, 13)\t5\n",
      "  (0, 200)\t4\n",
      "  (0, 277)\t1\n",
      "  (0, 274)\t5\n",
      "  (0, 21)\t1\n",
      "  (0, 181)\t1\n",
      "  (0, 146)\t1\n",
      "  (0, 300)\t1\n",
      "  (0, 129)\t1\n",
      "  (0, 148)\t1\n",
      "  (0, 273)\t1\n",
      "  (0, 147)\t1\n",
      "  (1, 22)\t1\n",
      "  (1, 263)\t1\n",
      "  (1, 207)\t1\n",
      "  (1, 25)\t1\n",
      "  (1, 115)\t1\n",
      "  (1, 234)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 309)\t1\n",
      "  (2, 10)\t1\n",
      "  :\t:\n",
      "  (28, 13)\t1\n",
      "  (28, 249)\t1\n",
      "  (28, 16)\t1\n",
      "  (28, 187)\t1\n",
      "  (28, 20)\t1\n",
      "  (28, 140)\t1\n",
      "  (28, 79)\t1\n",
      "  (28, 266)\t1\n",
      "  (28, 253)\t1\n",
      "  (29, 215)\t1\n",
      "  (29, 22)\t6\n",
      "  (29, 14)\t1\n",
      "  (29, 25)\t1\n",
      "  (29, 10)\t1\n",
      "  (29, 249)\t1\n",
      "  (29, 254)\t1\n",
      "  (29, 51)\t1\n",
      "  (29, 305)\t1\n",
      "  (29, 291)\t1\n",
      "  (29, 124)\t1\n",
      "  (29, 209)\t1\n",
      "  (29, 40)\t1\n",
      "  (29, 271)\t1\n",
      "  (29, 225)\t1\n",
      "  (29, 74)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(tokenizer=lambda x: x.split(' '))\n",
    "dtm = cv.fit_transform(x)\n",
    "print(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['github//^', '-//,', 'rhysd/unite-ruby-require//V', './/,', 'vim//N', '://,', 'a//D', 'unite//N', './/,', 'vim//N', 'source//N', 'for//P', 'searching//V', 'gems//N', 'to//P', 'require//V', '<//~', 'URL//N', '>//E', '<//~', 'URL//N', '>//E']\n",
      "[['github', '^'], ['-', ','], ['rhysd/unite-ruby-require', 'V'], ['.', ','], ['vim', 'N'], [':', ','], ['a', 'D'], ['unite', 'N'], ['.', ','], ['vim', 'N'], ['source', 'N'], ['for', 'P'], ['searching', 'V'], ['gems', 'N'], ['to', 'P'], ['require', 'V'], ['<', '~'], ['URL', 'N'], ['>', 'E'], ['<', '~'], ['URL', 'N'], ['>', 'E']]\n"
     ]
    }
   ],
   "source": [
    "slash_matcher = re.compile('(.+)//(.)')\n",
    "print(x[10].split(' '))\n",
    "print([y.split('//') for y in x[10].split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:loanword_adoption] *",
   "language": "python",
   "name": "conda-env-loanword_adoption-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
